\chapter{Conclusions and Future Work}

This chapter shall discuss the successes and failures of the project as a whole, both in terms of the successes of the individual models trained and the implications of these findings.
In light of this, the steps which can be taken in future work shall be explored in both the areas of lip reading with blendshape parameters and generative models to produce 3D temporal data for the purpose of lip reading tasks.

\section{Blendshape Classification}\label{sec:class_conc}
This report aimed to investigate as to whether 3D facial data includes information which is of use for word level speech prediction.
To achieve this, firstly a lower dimension representation of a 3D facial mesh was constructed with the use of Principal Component Analysis, referred to as the Blendshape Axes.
Each facial mesh could then be expressed as an interpolation along each Blendshape Axis from a template facial mesh, expressed by the Blendshape Parameters.
This representation of the 3D facial meshes allows for word level prediction for the 500 labels found in the Lip Reading Words Dataset with a mean test accuracy above 50\% for all model architectures investigated.
This demonstrates that firstly, there exists a strong correlation in the facial motion and the words spoken and secondly that this low dimension representation of the data effectively captures this data.

\subsection{Future Work}
The classification models are currently limited to word level prediction of a fixed sequence length and are unable to predict unknown word labels.
A future extension of this work would be to build a model which can perform character level prediction of variable sequence length.
This would allow for the prediction of entire sentences end to end, of variable sequence length.
In order to achieve this, a new dataset would have to be constructed containing sequence frames labelled on a character level.
This could be achieved with the existing dataset used by processing the audio data with a speech recognition system capable of character level prediction per frame, such as the DeepSpeech model.

Secondly, it should be noted that as a large amount of human lip reading relies on the context of the word in the current sentence, incorporating this context, either at a word or character level would likely prove beneficial to model performance.

Given that the classification models have successfully shown that there does exist a correlation between facial motion and speech, a practical application for these findings would be to apply visual speech prediction in a noisy environment where traditional audio speech recognition may perform poorly.
A suggested implementation of this would be to combine predictions from both audio speech recognition and visual speech recognition with the means of a depth camera.
The fusion of these two predictions produces an aggregate model, increasing the likelihood of correct predictions.

\section{Generative Models}
In addition to investigating the use of 3D facial meshes as a means of speech prediction, this report also investigated the use of a Generative Adversarial Network (GAN) to generate Blendshape Parameters driven by an audio input.
The primary incentives behind this were that the current acquisition of 3D temporal data is currently difficult which resulted in the use of existing audio driven facial animation models to be used to construct the dataset on which all models were trained.
This data however is limited in the variations of facial animation for given audio inputs.
With the intention of generating realistic facial animation from an audio input while having a larger variations in facial motion such that classification models may learn to generalise better to unseen data.

The model however proved unsuccessful in achieving this goal.
Generated samples appeared to be correlated with audio samples to human observers, but failed to contain features which are correlated to the words spoken within the audio itself when evaluated with the classification models trained beforehand.
Regardless of the failure of this generative model, generating synthetic data remains an area which should be further pursued while data acquisition of 3D facial scans retains a large barrier to entry.

\subsection{Future Work}
The generative and classification models in this report were trained on data not directly captured due to the difficulties of obtaining such data with the purpose of lip reading.
A primary area of work would be to directly obtain such a dataset, this would allow for a better assessment of the use of GANs as a means of generating synthetic data to augment this with.

The GAN trained in this report was trained with a Critic used to distinguish between real and fake data samples given an audio input and the real or fake generated Blendshape Parameters.
An extension of this would be to train the Generator on both the loss from the Critic and a pretrained classification model.
This may allow for the Generator to learn the features which are present in the data which are correlated to the speech in the audio.
However, this risks the Generator learning to trick the classification model without learning to generalise to unseen audio samples.
Given that the current mean test accuracy of the classification models trained in this report is around 50\%, this may not be an effective means to force the Generator to learn the features which it has currently failed to.

An alternate method of training the GAN would be to drive the generation from a character level conditional input, as opposed to audio driven conditional input.
This would require the model to be able to be trained on variable length input sequence, which in turn would require a dataset labelled on a character level as discussed in Section \ref{sec:class_conc}.
This could be trained either in combination with an audio input on without.

\documentclass[12pt]{article}
\input{notation}
\input{includes}

\begin{document}
\section{Principal Component Analysis}
Given a collection of measured data points in a high dimensional space, it is often desirable to reduce the dimensionality of such data to a lower dimensional latent space, this could be to aid to processing the data or enable visualisation.
In such instances, it is desirable for the mapping to the latent space to maintain as much information from the original data as possible.

Principal Component Analysis (PCA) aims to reduce the dimensionality of a collection of data points while maximising the variance in the latent space.
Alternatively, PCA aims to find a mapping from the original data to a new latent space which allow for the original data to be reconstructed with minimal error.
These two aims are in fact equivalent.

\subsection{Title?}
Let $\bm{x_i} \in \mathbb{R}^f$ represent a data point in $f$ dimensional space and $\bm{y_i} \in \mathbb{R}^d$ represent the point which $\bm{x_i}$ is transformed to by a linear transformation $\mat{W}$ equation (\ref{eq:pca_transformation}), where $d << f$.

\begin{equation} \label{eq:pca_transformation}
    \bm{y_i} = \mat{W}^\top \bm{x_i}
\end{equation}
\noindent
where,

\begin{equation*}
    \bm{y_i} = \begin{bmatrix} 
                y_{i1} \\
                \vdots \\
                y_{id} 
               \end{bmatrix},
    \quad
    \bm{x_i} = \begin{bmatrix} 
                \bm{x}_{i1} \\
                \vdots \\
                \bm{x}_{if} 
               \end{bmatrix},
    \quad
    \mat{W} = [
               \bm{w_1}, \dots, \bm{w_d}
              ],
    \quad
    \bm{w_i} = \begin{bmatrix} 
                w_{i1} \\
                \vdots \\
                w_{if} 
               \end{bmatrix}
\end{equation*}

\bigskip
\noindent
The optimal transformation $\mat{W}$ will maximise the variance in the data, where variance is expressed by equation (\ref{eq:variance}).
This then follows that the optimal solution is found by maximising equation (\ref{eq:pca_optimum_transformation}).
To prevent the trivial solution where $\bm{w}_k = \infty$, constrain a fixed magnitude $\mat{W}^\top \mat{W} = \mat{I}$.


\begin{equation} \label{eq:variance}
    \sigma_y^2 = \frac{1}{N} \sum_{i=1}^{N} (y_{ik} - \mu_k)^2
\end{equation}
\noindent
where,
\begin{equation*}
    \mu_k = \frac{1}{N} \sum_{i=1}^N y_{ik}
\end{equation*}

\begin{align*}
    \mat{W} &=\operatorname*{arg}\operatorname*{max}_\mat{W} 
                \frac{1}{N} \sum_{k=1}^{d} \sum_{i=1}^{N} 
                (y_{ik} - \mu_k)^2 \\
            &=\operatorname*{arg}\operatorname*{max}_\mat{W} 
                \frac{1}{N} \sum_{k=1}^{d} \sum_{i=1}^{N} 
                \bm{w}_k^\top 
                (\bm{x}_i - \bm{\mu})
                (\bm{x}_i - \bm{\mu})^\top 
                \bm{w}_k \\
            &=\operatorname*{arg}\operatorname*{max}_\mat{W} 
                \sum_{k=1}^{d}
                \bm{w}_k^\top 
                \mat{S}_t
                \bm{w}_k \\
\end{align*}
\noindent
where $\mat{S}_t$ is the covariance matrix,
\begin{equation} \label{eq:covariance}
    \mat{S}_t = \frac{1}{N} \sum_{i=1}^{N} 
                (\bm{x}_i - \bm{\mu})
                (\bm{x}_i - \bm{\mu})^\top 
\end{equation}
\noindent
and,
\begin{equation*}
    \bm{\mu} = \frac{1}{N} \sum_{i=1}^N \bm{x}_i
\end{equation*}


\begin{equation} \label{eq:pca_optimum_transformation}
    \mat{W} = \operatorname*{arg}\operatorname*{max}_\mat{W} 
          \tr[\mat{W}^\top \mat{S}_t \mat{W}],
          \quad
    \text{subject to} \quad 
    \mat{W}^\top \mat{W} = \mat{I}
\end{equation}

\bigskip
\noindent
By constructing the Lagangian from equation (\ref{eq:pca_optimum_transformation}) and solving the solution in equation (\ref{eq:pca_solved}) can be obtained.
From eigendecomposition, $\mat{W}$ has columns of columns of eigenvectors which correspond to the $d$ largest non-zero eigenvalues of the covariance matrix, $\mat{S}_t$.

\begin{equation*}
    L(\mat{W}, \mat{\Lambda}) = \tr[\mat{W}^\top \mat{S}_t \mat{W}] - \tr[\Lambda(\mat{W}^\top \mat{W} - \mat{I})]
\end{equation*}

\begin{equation*}
    \frac{\partial \tr[\mat{W}^\top \mat{S}_t \mat{W}]}{\partial \mat{W}} = 2 \mat{S}_t \mat{W},
    \quad
    \frac{\partial \tr[\Lambda(\mat{W}^\top \mat{W} - \mat{I})]}{\partial \mat{W}} = 2 \mat{W \Lambda}
\end{equation*}

\begin{equation*}
    L(\mat{W}, \mat{\Lambda}) = 0 
\end{equation*}

\begin{equation} \label{eq:pca_solved}
    \mat{S}_t \mat{W} = \mat{W \Lambda}
\end{equation}


\newpage
\bibliographystyle{unsrt}
\bibliography{ref}
\end{document}